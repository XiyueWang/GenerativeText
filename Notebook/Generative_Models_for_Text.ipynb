{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6c5lujfaVtl6"
   },
   "source": [
    "### USC ID: 7184-0277-30\n",
    "### Name: Xiyue Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1nB1X5p8VzrG"
   },
   "source": [
    "## Introduction\n",
    "In this problem, we are trying to build a generative model to mimic the writing style of prominent British Mathematician, Philosopher, prolific writer, and\n",
    "political activist, Bertrand Russell. \n",
    "\n",
    "The text materials will be used include: \n",
    "- The Problems of Philosophy\n",
    "- The Analysis of Mind\n",
    "- Mysticism and Logic and Other Essays\n",
    "- Our Knowledge of the External World as a Field for Scientific Method in\n",
    "Philosophy\n",
    "- The Analysis of Matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "bVOcoNyidUWZ",
    "outputId": "a146289d-9128-4f2f-d3e5-c0db1e0f9b78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import save_model\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0krSUNiWX1e"
   },
   "source": [
    "## (b) Download the books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MnKeCZLkWVwh"
   },
   "outputs": [],
   "source": [
    "# request txt files\n",
    "filepaths = ['http://www.gutenberg.org/cache/epub/5827/pg5827.txt', 'http://www.gutenberg.org/cache/epub/2529/pg2529.txt', \n",
    "             'http://www.gutenberg.org/files/25447/25447-0.txt', 'http://www.gutenberg.org/files/37090/37090-0.txt', \n",
    "             'https://archive.org/stream/in.ernet.dli.2015.221533/2015.221533.The-Analysis_djvu.txt']\n",
    "for filepath in filepaths:\n",
    "    r = requests.get(filepath)\n",
    "    with open('data.txt', 'a') as file:\n",
    "    file.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OWLFzJzyVy_m"
   },
   "outputs": [],
   "source": [
    "# read the text \n",
    "with open('data.txt', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XbT0S5G4gG_Z",
    "outputId": "b4dc6c75-d0bb-414d-cf3d-10b943288226"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lenth of the five books is: 2635771 characters.\n"
     ]
    }
   ],
   "source": [
    "print('The lenth of the five books is: {} characters.'.format(len(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xco_FIk6iObL"
   },
   "source": [
    "## (c) LSTM: Train an LSTM to mimic Russell’s style and thoughts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PmgFXPhoiORm"
   },
   "source": [
    "### i. Concatenate your text files to create a corpus of Russell’s writings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O6fkYlXFiNAK"
   },
   "outputs": [],
   "source": [
    "# remove punctuation and change to lowercase\n",
    "text = re.sub(r'[^a-zA-Z0-9]', ' ', text).lower().strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpvIDbAtixaa"
   },
   "source": [
    "### ii. Use a character-level representation for this model by using extended ASCII that has N = 256 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "hxh_2tEYjmfZ",
    "outputId": "e3c04ff5-1a7a-4300-c343-b1d15f6f147a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text has 37 unique character.\n",
      "\n",
      "The scaled encoder is: {'x': 0.0, '1': 0.027777777777777776, 'f': 0.05555555555555555, '2': 0.08333333333333333, 'o': 0.1111111111111111, 'h': 0.1388888888888889, 'e': 0.16666666666666666, '8': 0.19444444444444442, 'p': 0.2222222222222222, 'y': 0.25, 'i': 0.2777777777777778, 'c': 0.3055555555555555, '0': 0.3333333333333333, 'u': 0.3611111111111111, 'a': 0.38888888888888884, '9': 0.41666666666666663, 't': 0.4444444444444444, 'r': 0.4722222222222222, 'n': 0.5, 'b': 0.5277777777777778, 'v': 0.5555555555555556, '3': 0.5833333333333333, 's': 0.611111111111111, 'm': 0.6388888888888888, 'z': 0.6666666666666666, 'l': 0.6944444444444444, '5': 0.7222222222222222, 'd': 0.75, '7': 0.7777777777777777, 'g': 0.8055555555555555, 'q': 0.8333333333333333, 'j': 0.861111111111111, 'k': 0.8888888888888888, 'w': 0.9166666666666666, '6': 0.9444444444444444, '4': 0.9722222222222222, ' ': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# get the unique characters in the original text\n",
    "unique_char = set(text)\n",
    "encoded_char = {}\n",
    "for i, char in enumerate(unique_char):\n",
    "    encoded_char[char] = i\n",
    "print('The text has {} unique character.\\n'.format(len(unique_char)))\n",
    "\n",
    "# normalize the encoders between 0 and 1\n",
    "scaled_char = {}\n",
    "scaler = MinMaxScaler()\n",
    "norm_value = scaler.fit_transform(np.array(list(encoded_char.values())).reshape(-1, 1))\n",
    "for i in range(len(norm_value)):\n",
    "    scaled_char[list(encoded_char.keys())[i]] = norm_value[i][0]\n",
    "print('The scaled encoder is: {}'.format(scaled_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOidFMEKoOw3"
   },
   "source": [
    "### iii. Choose a window size, e.g., W = 100.\n",
    "### iv. Inputs to the network will be the first W −1 = 99 characters of each sequence, and the output of the network will be the Wth character of the sequence. Basically, we are training the network to predict each character using the 99 characters that precede it. Slide the window in strides of S = 1 on the text. For example, if W = 5 and S = 1 and we want to train the network with the sequence ABRACADABRA, The first input to the network will be ABRA and the corresponding output will be C. The second input will be BRAC and the second output will be A, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "64WLHhneid6Q"
   },
   "outputs": [],
   "source": [
    "## choose a window size of 100\n",
    "w = 100\n",
    "X = []\n",
    "y = []\n",
    "for i in range(0, len(text)-w):\n",
    "    temp = text[i:i+w-1]\n",
    "    temp_label = text[i+w]\n",
    "    X.append([scaled_char[char] for char in temp])\n",
    "    y.append(encoded_char[temp_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "26ds7aCLIZpm",
    "outputId": "ca2ea2bb-58b5-442f-e5e2-4a549e486b52"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2635664, 99, 1), (2635664, 37))"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape the dataset and change the label to one hot label\n",
    "n = len(X)\n",
    "X_trans = np.reshape(X, (n, w-1, 1))\n",
    "y_trans = to_categorical(y) # keras don't take multilabel target, has to do one hot encoding first\n",
    "X_trans.shape, y_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ETQACrFKL--X"
   },
   "outputs": [],
   "source": [
    "# store the X and y data\n",
    "with open('X.pickle', 'wb') as file:\n",
    "    pickle.dump(X, file)\n",
    "with open('y.pickle', 'wb') as file:\n",
    "    pickle.dump(y, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MBx2HC1YLk6f"
   },
   "source": [
    "### v. Note that the output has to be encoded using a one-hot encoding scheme with N = 256 (or less) elements. This means that the network reads integers, but outputs a vector of N = 256 (or less) elements.\n",
    "### vi. Use a single hidden layer for the LSTM with N = 256 (or less) memory units.\n",
    "### vii. Use a Softmax output layer to yield a probability prediction for each of the characters between 0 and 1. This is actually a character classification problem with N classes. Choose log loss (cross entropy) as the objective function for the network.\n",
    "### viii. We do not use a test dataset. We are using the whole training dataset to learn the probability of each character in a sequence. We are not seeking for a very accurate model. Instead we are interested in a generalization of the dataset that can mimic the gist of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybS7WCqDKwy0"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "memory_units = 256\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=memory_units, input_shape=(X_trans.shape[1], X_trans.shape[2])))\n",
    "model.add(Dense(y_trans.shape[1], activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SkTikml6OfAl"
   },
   "source": [
    "### iX. Choose a reasonable number of epochs for training, considering your computational power (e.g., 30, although the network will need more epochs to yield a better model).\n",
    "### X. Use model checkpointing to keep the network weights to determine each time an improvement in loss is observed at the end of the epoch. Find the best set of weights in terms of loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "edg6lrd3Oe0w",
    "outputId": "48d52d5e-a2a4-4182-de1b-70c910f2c49d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2635666/2635666 [==============================] - 821s 312us/step - loss: 2.8227 - accuracy: 0.2162\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2635666/2635666 [==============================] - 819s 311us/step - loss: 2.7739 - accuracy: 0.2166\n",
      "Epoch 3/30\n",
      "2635666/2635666 [==============================] - 845s 321us/step - loss: 2.7641 - accuracy: 0.2166\n",
      "Epoch 4/30\n",
      "2635666/2635666 [==============================] - 803s 305us/step - loss: 2.7543 - accuracy: 0.2166\n",
      "Epoch 5/30\n",
      "2635666/2635666 [==============================] - 808s 307us/step - loss: 2.7417 - accuracy: 0.2176\n",
      "Epoch 6/30\n",
      "2635666/2635666 [==============================] - 828s 314us/step - loss: 2.7324 - accuracy: 0.2196\n",
      "Epoch 7/30\n",
      "2635666/2635666 [==============================] - 893s 339us/step - loss: 2.7233 - accuracy: 0.2214\n",
      "Epoch 8/30\n",
      "2635666/2635666 [==============================] - 885s 336us/step - loss: 2.7137 - accuracy: 0.2239\n",
      "Epoch 9/30\n",
      "2635666/2635666 [==============================] - 803s 305us/step - loss: 2.7034 - accuracy: 0.2266\n",
      "Epoch 10/30\n",
      "2635666/2635666 [==============================] - 791s 300us/step - loss: 2.6922 - accuracy: 0.2296\n",
      "Epoch 11/30\n",
      "2635666/2635666 [==============================] - 796s 302us/step - loss: 2.6792 - accuracy: 0.2326\n",
      "Epoch 12/30\n",
      "2635666/2635666 [==============================] - 795s 302us/step - loss: 2.6652 - accuracy: 0.2361\n",
      "Epoch 13/30\n",
      "2635666/2635666 [==============================] - 797s 302us/step - loss: 2.6502 - accuracy: 0.2400\n",
      "Epoch 14/30\n",
      "2635666/2635666 [==============================] - 792s 300us/step - loss: 2.6348 - accuracy: 0.2435\n",
      "Epoch 15/30\n",
      "2635666/2635666 [==============================] - 789s 299us/step - loss: 2.6191 - accuracy: 0.2476\n",
      "Epoch 16/30\n",
      "2635666/2635666 [==============================] - 789s 299us/step - loss: 2.6034 - accuracy: 0.2516\n",
      "Epoch 17/30\n",
      "2635666/2635666 [==============================] - 796s 302us/step - loss: 2.5880 - accuracy: 0.2561\n",
      "Epoch 18/30\n",
      "2635666/2635666 [==============================] - 795s 301us/step - loss: 2.5729 - accuracy: 0.2609\n",
      "Epoch 19/30\n",
      "2635666/2635666 [==============================] - 811s 308us/step - loss: 2.5585 - accuracy: 0.2653\n",
      "Epoch 20/30\n",
      "2635666/2635666 [==============================] - 796s 302us/step - loss: 2.5443 - accuracy: 0.2691\n",
      "Epoch 21/30\n",
      "2635666/2635666 [==============================] - 797s 303us/step - loss: 2.5315 - accuracy: 0.2732\n",
      "Epoch 22/30\n",
      "2635666/2635666 [==============================] - 808s 306us/step - loss: 2.5182 - accuracy: 0.2775\n",
      "Epoch 23/30\n",
      "2635666/2635666 [==============================] - 813s 308us/step - loss: 2.5058 - accuracy: 0.2816\n",
      "Epoch 24/30\n",
      "2635666/2635666 [==============================] - 821s 312us/step - loss: 2.4942 - accuracy: 0.2858\n",
      "Epoch 25/30\n",
      "2635666/2635666 [==============================] - 826s 313us/step - loss: 2.4826 - accuracy: 0.2894\n",
      "Epoch 26/30\n",
      "2635666/2635666 [==============================] - 816s 310us/step - loss: 2.4716 - accuracy: 0.2928\n",
      "Epoch 27/30\n",
      "2635666/2635666 [==============================] - 815s 309us/step - loss: 2.4609 - accuracy: 0.2965\n",
      "Epoch 28/30\n",
      "2635666/2635666 [==============================] - 819s 311us/step - loss: 2.4505 - accuracy: 0.2997\n",
      "Epoch 29/30\n",
      "2635666/2635666 [==============================] - 818s 310us/step - loss: 2.4405 - accuracy: 0.3031\n",
      "Epoch 30/30\n",
      "2635666/2635666 [==============================] - 825s 313us/step - loss: 2.4311 - accuracy: 0.3064\n"
     ]
    }
   ],
   "source": [
    "# adapted from https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "output_dir = \"./checkpoints\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "checkpoint_filepath = os.path.join(output_dir, 'ck_{epoch:02d}.hdf5')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath = checkpoint_filepath,\n",
    "    monitor='loss',\n",
    "    save_weights_only= True,\n",
    "    mode='min'\n",
    ")\n",
    "early_stopping = EarlyStopping(patience = 5, min_delta = 1e-4)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "\n",
    "# initiate model\n",
    "check_points = model.fit(X_trans, y_trans, epochs=30, batch_size= 500, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UC0ScJok86tY"
   },
   "source": [
    "### v. Use model checkpointing to keep the network weights to determine each time an improvement in loss is observed at the end of the epoch. Find the best set of weights in terms of loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H9sP2k8OP-Ps"
   },
   "outputs": [],
   "source": [
    "best_model = output_dir +'/ck_30.hdf5'\n",
    "model.load_weights(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I314whAm96Q9"
   },
   "source": [
    "### vi. Use the network with the best weights to generate 1000 characters, using the following text as initialization of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJIeuMiI9I8X"
   },
   "outputs": [],
   "source": [
    "text = 'There are those who take mental phenomena naively, just as they would physical phenomena.This school of psychologists tends not to emphasize the object.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-b6zZIg-3-c"
   },
   "outputs": [],
   "source": [
    "# remove punctuation and change to lowercase\n",
    "text = re.sub(r'[^a-zA-Z0-9]', ' ', text).lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H0BNKng-_UK7"
   },
   "outputs": [],
   "source": [
    "# predict the characters\n",
    "while len(text) <= 1000:\n",
    "  # transfer the text into scaled floats\n",
    "    X_test = []\n",
    "    for i in range(0, len(text)-99):\n",
    "    temp = text[i:i+99]\n",
    "    X_test.append([scaled_char[char] for char in temp])\n",
    "    X_test_trans = np.reshape(X_test, (len(X_test), 99, 1))\n",
    "    pred=model.predict(X_test_trans)\n",
    "    for j in range(len(X_test)):\n",
    "        pred_char = list(encoded_char.keys())[pred[j, :].argmax()] # get the corresponding char\n",
    "        text+=pred_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "D6g0Fo01_Vcz",
    "outputId": "3b8eba74-ba8a-41b8-9e09-72b9a4d25ebb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are those who take mental phenomena naively  just as they would physical phenomena this school of psychologists tends not to emphasize the object     a  a   a    a  h  a  he  a  het  a  hete  a  heter  a  heterl  a  heterlo  a  heterlog  a  heterlogy  a  heterlogy   a  heterlogy a  a  heterlogy a   a  heterlogy a    a  heterlogy a     a  heterlogy a   h  a  heterlogy a   he  a  heterlogy a   he   a  heterlogy a   he    a  heterlogy a   he     a  heterlogy a   he      a  heterlogy a   he    a  a  heterlogy a   he    a   a  heterlogy a   he    a    a  heterlogy a   he    a     a  heterlogy a   he    a      a  heterlogy a   he    a       a  heterlogy a   he    a        a  heterlogy a   he    a      h  a  heterlogy a   he    a      h   a  heterlogy a   he    a      h e  a  heterlogy a   he    a      h e   a  heterlogy a   he    a      h e h  a  heterlogy a   he    a      h e hc  a  heterlogy a   he    a      h e hc   a  heterlogy a   he    a      h e hc h  a  heterlogy a   he    a      h e hc h   a  heterlogy a   he    a      h e hc h t  a  heterlogy a   he    a      h e hc h te  a  heterlogy a   he    a      h e hc h tee  a  heterlogy a   he    a      h e hc h tee   a  heterlogy a   he    a      h e hc h tee    a  heterlogy a   he    a      h e hc h tee  a  a  heterlogy a   he    a      h e hc h tee  ar  a  heterlogy a   he    a      h e hc h tee  are  a  heterlogy a   he    a      h e hc h tee  arer  a  heterlogy a   he    a      h e hc h tee  arert  a  heterlogy a   he    a      h e hc h tee  arert \n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjYo1P9wG5wx"
   },
   "source": [
    "### vii. Extra Practice: Use one-hot encoding for the input sequence. Use a large number of epochs, e.g., 150. Add dropout to the network, and use a deeper LSTM (e.g. with 3 or more layers). Generate 3000 characters using the above initialization and report if you get more meaningful text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3sQuVo_Lp0D"
   },
   "outputs": [],
   "source": [
    "# try one book for computation power \n",
    "filepath = 'http://www.gutenberg.org/cache/epub/5827/pg5827.txt'\n",
    "r = requests.get(filepath)\n",
    "with open('phi.text', 'w') as file:\n",
    "    file.write(r.text)\n",
    "with open('phi.text', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "eT9oZI-wFt2m",
    "outputId": "22cd388a-a2c7-4e45-b125-534ef414f5b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text has 37 unique character.\n"
     ]
    }
   ],
   "source": [
    "# remove punctuation and change to lowercase\n",
    "text = re.sub(r'[^a-zA-Z0-9]', ' ', text).lower().strip()\n",
    "# get the unique characters in the original text\n",
    "unique_char = set(text)\n",
    "encoded_char = {}\n",
    "for i, char in enumerate(unique_char):\n",
    "    encoded_char[char] = i\n",
    "print('The text has {} unique character.'.format(len(unique_char)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_65jW7MHL4x"
   },
   "outputs": [],
   "source": [
    "## choose a window size of 100\n",
    "w = 100\n",
    "X = []\n",
    "y = []\n",
    "for i in range(0, len(text)-w):\n",
    "    temp = text[i:i+w-1]\n",
    "    temp_label = text[i+w]\n",
    "    X.append([encoded_char[char] for char in temp])\n",
    "    y.append(encoded_char[temp_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "znA2HJIxOIox",
    "outputId": "2e05b877-bf73-4b74-cf45-cb34396a17c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((264982, 99, 1), (264982, 37))"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape the dataset and change the label to one hot label\n",
    "n = len(X)\n",
    "X_trans = np.reshape(X, (n, w-1, 1))\n",
    "y_trans = to_categorical(y) # keras don't take multilabel target, has to do one hot encoding first\n",
    "X_trans.shape, y_trans.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kY0KRA78HSJC"
   },
   "outputs": [],
   "source": [
    "# build the model with 3 layers of SMTM \n",
    "memory_units = 256\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(units=memory_units, input_shape=(X_trans.shape[1], X_trans.shape[2]), return_sequences=True))\n",
    "model2.add(LSTM(units=memory_units, return_sequences=True))\n",
    "model2.add(LSTM(units=memory_units, return_sequences=False))\n",
    "model2.add(Dense(y_trans.shape[1], activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model2.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jvRr7bq4HvUN",
    "outputId": "192ca78a-674d-450f-a2ea-e7977447ec4e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "264982/264982 [==============================] - 105s 397us/step - loss: 2.8928 - accuracy: 0.2013\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/callbacks/callbacks.py:846: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264982/264982 [==============================] - 106s 398us/step - loss: 2.8140 - accuracy: 0.2032\n",
      "Epoch 3/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.8124 - accuracy: 0.2032\n",
      "Epoch 4/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.8108 - accuracy: 0.2032\n",
      "Epoch 5/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.8092 - accuracy: 0.2032\n",
      "Epoch 6/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.8056 - accuracy: 0.2032\n",
      "Epoch 7/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.7992 - accuracy: 0.2031\n",
      "Epoch 8/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.7880 - accuracy: 0.2031\n",
      "Epoch 9/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.7728 - accuracy: 0.2057\n",
      "Epoch 10/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.7580 - accuracy: 0.2085\n",
      "Epoch 11/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.7439 - accuracy: 0.2109\n",
      "Epoch 12/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.7321 - accuracy: 0.2155\n",
      "Epoch 13/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.7190 - accuracy: 0.2193\n",
      "Epoch 14/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.7067 - accuracy: 0.2211\n",
      "Epoch 15/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.6940 - accuracy: 0.2228\n",
      "Epoch 16/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.6818 - accuracy: 0.2243\n",
      "Epoch 17/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.6699 - accuracy: 0.2256\n",
      "Epoch 18/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.6574 - accuracy: 0.2279\n",
      "Epoch 19/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.6456 - accuracy: 0.2316\n",
      "Epoch 20/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.6333 - accuracy: 0.2357\n",
      "Epoch 21/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.6218 - accuracy: 0.2391\n",
      "Epoch 22/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.6098 - accuracy: 0.2422\n",
      "Epoch 23/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.5979 - accuracy: 0.2450\n",
      "Epoch 24/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.5861 - accuracy: 0.2478\n",
      "Epoch 25/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.5741 - accuracy: 0.2504\n",
      "Epoch 26/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.5625 - accuracy: 0.2526\n",
      "Epoch 27/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.5517 - accuracy: 0.2542\n",
      "Epoch 28/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.5405 - accuracy: 0.2565\n",
      "Epoch 29/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.5295 - accuracy: 0.2597\n",
      "Epoch 30/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.5186 - accuracy: 0.2628\n",
      "Epoch 31/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.5080 - accuracy: 0.2672\n",
      "Epoch 32/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.4979 - accuracy: 0.2712\n",
      "Epoch 33/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.4869 - accuracy: 0.2755\n",
      "Epoch 34/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.4763 - accuracy: 0.2788\n",
      "Epoch 35/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.4660 - accuracy: 0.2816\n",
      "Epoch 36/100\n",
      "264982/264982 [==============================] - 106s 398us/step - loss: 2.4563 - accuracy: 0.2844\n",
      "Epoch 37/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.4465 - accuracy: 0.2879\n",
      "Epoch 38/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.4363 - accuracy: 0.2907\n",
      "Epoch 39/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.4262 - accuracy: 0.2936\n",
      "Epoch 40/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.4161 - accuracy: 0.2973\n",
      "Epoch 41/100\n",
      "264982/264982 [==============================] - 107s 402us/step - loss: 2.4067 - accuracy: 0.3003\n",
      "Epoch 42/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.3973 - accuracy: 0.3036\n",
      "Epoch 43/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.3872 - accuracy: 0.3065\n",
      "Epoch 44/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.3781 - accuracy: 0.3097\n",
      "Epoch 45/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.3689 - accuracy: 0.3131\n",
      "Epoch 46/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.3594 - accuracy: 0.3162\n",
      "Epoch 47/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.3503 - accuracy: 0.3191\n",
      "Epoch 48/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.3405 - accuracy: 0.3223\n",
      "Epoch 49/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.3321 - accuracy: 0.3248\n",
      "Epoch 50/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.3235 - accuracy: 0.3281\n",
      "Epoch 51/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.3142 - accuracy: 0.3310\n",
      "Epoch 52/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.3064 - accuracy: 0.3335\n",
      "Epoch 53/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.2977 - accuracy: 0.3369\n",
      "Epoch 54/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.2897 - accuracy: 0.3394\n",
      "Epoch 55/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.2813 - accuracy: 0.3413\n",
      "Epoch 56/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.2731 - accuracy: 0.3439\n",
      "Epoch 57/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.2649 - accuracy: 0.3469\n",
      "Epoch 58/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.2575 - accuracy: 0.3492\n",
      "Epoch 59/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.2500 - accuracy: 0.3520\n",
      "Epoch 60/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.2418 - accuracy: 0.3544\n",
      "Epoch 61/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.2348 - accuracy: 0.3567\n",
      "Epoch 62/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.2272 - accuracy: 0.3588\n",
      "Epoch 63/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.2198 - accuracy: 0.3613\n",
      "Epoch 64/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.2126 - accuracy: 0.3638\n",
      "Epoch 65/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.2058 - accuracy: 0.3663\n",
      "Epoch 66/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.1990 - accuracy: 0.3688\n",
      "Epoch 67/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.1917 - accuracy: 0.3704\n",
      "Epoch 68/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.1847 - accuracy: 0.3732\n",
      "Epoch 69/100\n",
      "264982/264982 [==============================] - 106s 402us/step - loss: 2.1782 - accuracy: 0.3753\n",
      "Epoch 70/100\n",
      "264982/264982 [==============================] - 107s 402us/step - loss: 2.1715 - accuracy: 0.3774\n",
      "Epoch 71/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.1650 - accuracy: 0.3798\n",
      "Epoch 72/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.1593 - accuracy: 0.3814\n",
      "Epoch 73/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.1517 - accuracy: 0.3837\n",
      "Epoch 74/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.1460 - accuracy: 0.3854\n",
      "Epoch 75/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.1399 - accuracy: 0.3874\n",
      "Epoch 76/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.1333 - accuracy: 0.3892\n",
      "Epoch 77/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.1272 - accuracy: 0.3911\n",
      "Epoch 78/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.1210 - accuracy: 0.3928\n",
      "Epoch 79/100\n",
      "264982/264982 [==============================] - 107s 403us/step - loss: 2.1159 - accuracy: 0.3945\n",
      "Epoch 80/100\n",
      "264982/264982 [==============================] - 107s 404us/step - loss: 2.1097 - accuracy: 0.3958\n",
      "Epoch 81/100\n",
      "264982/264982 [==============================] - 107s 402us/step - loss: 2.1040 - accuracy: 0.3973\n",
      "Epoch 82/100\n",
      "264982/264982 [==============================] - 107s 404us/step - loss: 2.0986 - accuracy: 0.3995\n",
      "Epoch 83/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.0927 - accuracy: 0.4013\n",
      "Epoch 84/100\n",
      "264982/264982 [==============================] - 107s 402us/step - loss: 2.0870 - accuracy: 0.4024\n",
      "Epoch 85/100\n",
      "264982/264982 [==============================] - 107s 402us/step - loss: 2.0818 - accuracy: 0.4037\n",
      "Epoch 86/100\n",
      "264982/264982 [==============================] - 107s 403us/step - loss: 2.0761 - accuracy: 0.4055\n",
      "Epoch 87/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.0710 - accuracy: 0.4075\n",
      "Epoch 88/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.0651 - accuracy: 0.4088\n",
      "Epoch 89/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.0604 - accuracy: 0.4111\n",
      "Epoch 90/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.0546 - accuracy: 0.4120\n",
      "Epoch 91/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.0495 - accuracy: 0.4137\n",
      "Epoch 92/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.0446 - accuracy: 0.4147\n",
      "Epoch 93/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.0398 - accuracy: 0.4165\n",
      "Epoch 94/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.0351 - accuracy: 0.4180\n",
      "Epoch 95/100\n",
      "264982/264982 [==============================] - 106s 399us/step - loss: 2.0289 - accuracy: 0.4198\n",
      "Epoch 96/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.0251 - accuracy: 0.4205\n",
      "Epoch 97/100\n",
      "264982/264982 [==============================] - 107s 402us/step - loss: 2.0201 - accuracy: 0.4220\n",
      "Epoch 98/100\n",
      "264982/264982 [==============================] - 106s 402us/step - loss: 2.0145 - accuracy: 0.4234\n",
      "Epoch 99/100\n",
      "264982/264982 [==============================] - 106s 401us/step - loss: 2.0107 - accuracy: 0.4246\n",
      "Epoch 100/100\n",
      "264982/264982 [==============================] - 106s 400us/step - loss: 2.0052 - accuracy: 0.4261\n"
     ]
    }
   ],
   "source": [
    "# fit the model with 100 ephochs\n",
    "# adapted from https://machinelearningmastery.com/check-point-deep-learning-models-keras/\n",
    "output_dir = \"./checkpoints2\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "    \n",
    "checkpoint_filepath = os.path.join(output_dir, 'ck_{epoch:02d}.hdf5')\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath = checkpoint_filepath,\n",
    "    monitor='loss',\n",
    "    save_weights_only= True,\n",
    "    mode='min'\n",
    ")\n",
    "early_stopping = EarlyStopping(patience = 5, min_delta = 1e-4)\n",
    "\n",
    "callbacks_list = [checkpoint, early_stopping]\n",
    "\n",
    "# initiate model\n",
    "check_points = model2.fit(X_trans, y_trans, epochs=100, batch_size=2400, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlklBq21IKJT"
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "best_model = output_dir +'/ck_100.hdf5'\n",
    "model2.load_weights(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYoLfmiCIuuW"
   },
   "outputs": [],
   "source": [
    "# predict the characters\n",
    "text = 'There are those who take mental phenomena naively, just as they would physical phenomena. This school of psychologists tends not to emphasize the object.'\n",
    "# remove punctuation and change to lowercase\n",
    "text = re.sub(r'[^a-zA-Z0-9]', ' ', text).lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rXumpf9X3RXc"
   },
   "outputs": [],
   "source": [
    "while (len(text) <= 3000):\n",
    "  # transfer the text into scaled floats\n",
    "    X_test = []\n",
    "    for i in range(0, len(text)-99):\n",
    "    temp = text[i:i+99]\n",
    "    X_test.append([encoded_char[char] for char in temp])\n",
    "    X_test_trans = np.reshape(X_test, (len(X_test), 99, 1))\n",
    "    pred=model2.predict(X_test_trans)\n",
    "    for j in range(len(X_test)):\n",
    "        pred_char = list(encoded_char.keys())[pred[j, :].argmax()] # get the corresponding char\n",
    "        text+=pred_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "lxYORiSj3jJ7",
    "outputId": "1cc3dca5-9ce8-4eb2-edc2-4f5d0b2ec89c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are those who take mental phenomena naively  just as they would physical phenomena  this school of psychologists tends not to emphasize the object         a   a    a t   a th   a thi   a thit   a thitr   a thitr    a thitr     a thitr      a thitr       a thitr        a thitr         a thitr      a   a thitr      a    a thitr      a     a thitr      a      a thitr      a   f   a thitr      a   fe   a thitr      a   fes   a thitr      a   fese   a thitr      a   fese    a thitr      a   fese a   a thitr      a   fese ao   a thitr      a   fese ao    a thitr      a   fese ao     a thitr      a   fese ao  b   a thitr      a   fese ao  be   a thitr      a   fese ao  bee   a thitr      a   fese ao  beet   a thitr      a   fese ao  beete   a thitr      a   fese ao  beeteo   a thitr      a   fese ao  beeteot   a thitr      a   fese ao  beeteotr   a thitr      a   fese ao  beeteotr    a thitr      a   fese ao  beeteotr     a thitr      a   fese ao  beeteotr  b   a thitr      a   fese ao  beeteotr  bl   a thitr      a   fese ao  beeteotr  bl    a thitr      a   fese ao  beeteotr  bl     a thitr      a   fese ao  beeteotr  bl  b   a thitr      a   fese ao  beeteotr  bl  be   a thitr      a   fese ao  beeteotr  bl  bet   a thitr      a   fese ao  beeteotr  bl  bete   a thitr      a   fese ao  beeteotr  bl  betee   a thitr      a   fese ao  beeteotr  bl  beteej   a thitr      a   fese ao  beeteotr  bl  beteeje   a thitr      a   fese ao  beeteotr  bl  beteejec   a thitr      a   fese ao  beeteotr  bl  beteeject   a thitr      a   fese ao  beeteotr  bl  beteeject          a   a    a t   a th   a thi   a thit   a thitr   a thitr    a thitr     a thitr      a thitr       a thitr        a thitr         a thitr      a   a thitr      a    a thitr      a     a thitr      a      a thitr      a   f   a thitr      a   fe   a thitr      a   fes   a thitr      a   fese   a thitr      a   fese    a thitr      a   fese a   a thitr      a   fese ao   a thitr      a   fese ao    a thitr      a   fese ao     a thitr      a   fese ao  b   a thitr      a   fese ao  be   a thitr      a   fese ao  bee   a thitr      a   fese ao  beet   a thitr      a   fese ao  beete   a thitr      a   fese ao  beeteo   a thitr      a   fese ao  beeteot   a thitr      a   fese ao  beeteotr   a thitr      a   fese ao  beeteotr    a thitr      a   fese ao  beeteotr     a thitr      a   fese ao  beeteotr  b   a thitr      a   fese ao  beeteotr  bl   a thitr      a   fese ao  beeteotr  bl    a thitr      a   fese ao  beeteotr  bl     a thitr      a   fese ao  beeteotr  bl  b   a thitr      a   fese ao  beeteotr  bl  be   a thitr      a   fese ao  beeteotr  bl  bet   a thitr      a   fese ao  beeteotr  bl  bete   a thitr      a   fese ao  beeteotr  bl  betee   a thitr      a   fese ao  beeteotr  bl  beteej   a thitr      a   fese ao  beeteotr  bl  beteeje   a thitr      a   fese ao  beeteotr  bl  beteejec   a thitr      a   fese ao  beeteotr  bl  beteeject   a thitr      a   fese ao  beeteotr  bl  beteeject    a thitr      a   fese ao  beeteotr  bl  beteeject     a thitr      a   fese ao  beeteotr  bl  beteeject  a   a thitr      a   fese ao  beeteotr  bl  beteeject  an   a thitr      a   fese ao  beeteotr  bl  beteeject  anh   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh    a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     a thitr      a   fese ao  beeteotr  bl  beteeject  anhh      a thitr      a   fese ao  beeteotr  bl  beteeject  anhh       a thitr      a   fese ao  beeteotr  bl  beteeject  anhh        a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d    a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i    a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i     a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  r   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro    a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro     a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  d   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  da   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dai   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dair   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo    a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo d   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo do   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doi   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doin   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinh   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinho   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoo   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhooc   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhooco   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoi   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoin   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoing   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoing    a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoing o   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoing oo   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoing oo    a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoing oo o   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoing oo oi   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoing oo oin   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoing oo oing   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoing oo oing    a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoing oo oing o   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoing oo oing oh   a thitr      a   fese ao  beeteotr  bl  beteeject  anhh     d i  ro  dairo doinhoocoing oo oing oh \n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6pE8xdLW4EF_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Generative Models for Text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
